{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to read Paid Parking Occupancy Data and extract unique SourceElementKey and Location\n",
    "\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# Constants\n",
    "CHUNK_SIZE = 5000000  # 5 million rows\n",
    "INPUT_FILE = 'PaidParkingOccupancyData_2015.csv'\n",
    "OUTPUT_FILE_TEMPLATE = 'unique_SourceElementKey_Location_chunk_{}.csv'\n",
    "\n",
    "def process_chunk(chunk_id, chunk):\n",
    "    unique_data = chunk.drop_duplicates(subset=['SourceElementKey'])[['SourceElementKey', 'Location']]\n",
    "    unique_data.to_csv(OUTPUT_FILE_TEMPLATE.format(chunk_id), index=False)\n",
    "    print(f\"Processed chunk {chunk_id}\")\n",
    "\n",
    "def divide_and_process_file(input_file):\n",
    "    chunk_id = 1\n",
    "    for chunk in pd.read_csv(input_file, chunksize=CHUNK_SIZE):\n",
    "        process_chunk(chunk_id, chunk)\n",
    "        chunk_id += 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Use ThreadPoolExecutor to utilize I/O bound task of reading from disk\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n",
    "        # Use ProcessPoolExecutor for CPU bound task of processing data\n",
    "        with concurrent.futures.ProcessPoolExecutor() as process_executor:\n",
    "            futures = []\n",
    "            chunk_id = 1\n",
    "            for chunk in pd.read_csv(INPUT_FILE, chunksize=CHUNK_SIZE):\n",
    "                futures.append(process_executor.submit(process_chunk, chunk_id, chunk))\n",
    "                chunk_id += 1\n",
    "\n",
    "            # Wait for all futures to complete processin            concurrent.futures.wait(futures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "def read_and_unique(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Assuming 'SourceElementKey' and 'Location' are the only columns in the chunk files\n",
    "    return df.drop_duplicates(subset=['SourceElementKey'])\n",
    "\n",
    "def combine_unique_files(file_paths, output_file):\n",
    "    # Process the files in parallel and concatenate the results\n",
    "    with ProcessPoolExecutor(max_workers=cpu_count()) as executor:\n",
    "        dataframes = list(executor.map(read_and_unique, file_paths))\n",
    "    combined_df = pd.concat(dataframes)\n",
    "    # Drop duplicates again after concatenation to ensure uniqueness\n",
    "    final_unique_df = combined_df.drop_duplicates(subset=['SourceElementKey'])\n",
    "    final_unique_df.to_csv(output_file, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate the list of chunk file paths\n",
    "    chunk_files = [f'unique_SourceElementKey_Location_chunk_{i}.csv' for i in range(1, 56)]\n",
    "    \n",
    "    # Specify the output file name\n",
    "    output_file = 'UniqueSourceElementKey_Lat_Long_Dask.csv'\n",
    "    \n",
    "    # Call the function to combine the files\n",
    "    combine_unique_files(chunk_files, output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "def get_osm_place_class_and_type(lon, lat):\n",
    "    url = 'https://nominatim.openstreetmap.org/reverse'\n",
    "    params = {'lat': lat, 'lon': lon, 'format': 'json'}\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data.get('class', ''), data.get('type', '')\n",
    "    else:\n",
    "        return '', ''\n",
    "\n",
    "def add_class_type_to_csv(input_filename, output_filename):\n",
    "    # Read the input file\n",
    "    data = pd.read_csv(input_filename)\n",
    "    \n",
    "    # Extract lon and lat from the 'Location' column\n",
    "    data[['lon', 'lat']] = data['Location'].str.extract(r'POINT \\(([^ ]+) ([^ ]+)\\)')\n",
    "    \n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    \n",
    "    # Define a new function that includes a counter for progress\n",
    "    def apply_get_osm_place_class_and_type(row):\n",
    "        nonlocal counter\n",
    "        counter += 1\n",
    "        print(f\"Processing row {counter}/{len(data)}\")\n",
    "        return get_osm_place_class_and_type(row['lon'], row['lat'])\n",
    "    \n",
    "    # Apply the function to each row\n",
    "    data[['class', 'type']] = data.apply(apply_get_osm_place_class_and_type, axis=1, result_type='expand')\n",
    "    \n",
    "    # Write to the output file\n",
    "    data.to_csv(output_filename, index=False)\n",
    "    \n",
    "    print(\"Finished processing rows.\")\n",
    "    print(f\"Total rows modified: {counter}\")\n",
    "\n",
    "\n",
    "input_csv_file = \"UniqueSourceElementKey_Lat_Long_Dask.csv\"\n",
    "output_csv_file = 'api_UniqueSourceElementKey_Lat_Long_Dask.csv'\n",
    "\n",
    "# Apply the function\n",
    "add_class_type_to_csv(input_csv_file, output_csv_file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
